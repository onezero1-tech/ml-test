import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np

class TextDataset(Dataset):
    def __init__(self, texts, labels, word_to_idx, max_len):
        self.texts = texts
        self.labels = labels
        self.word_to_idx = word_to_idx
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # Convert text to sequence of indices
        text_indices = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in text]
        text_indices = text_indices[:self.max_len] + [self.word_to_idx['<PAD>']] * (self.max_len - len(text_indices))
        
        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)
class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_classes, dropout):
        super(TextCNN, self).__init__()
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # Convolutional layers
        self.convs = nn.ModuleList([
            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embed_dim)) 
            for fs in filter_sizes
        ])
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
        
        # Fully connected layer
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
    
    def forward(self, x):
        # x: (batch_size, seq_len)
        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        x = x.unsqueeze(1)  # (batch_size, 1, seq_len, embed_dim)
        
        # Apply convolutional layers
        x = [nn.functional.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, num_filters, seq_len - filter_size + 1)] * len(filter_sizes)
        
        # Apply max pooling
        x = [nn.functional.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in x]  # [(batch_size, num_filters)] * len(filter_sizes)
        
        # Concatenate the results
        x = torch.cat(x, 1)  # (batch_size, num_filters * len(filter_sizes))
        
        # Apply dropout
        x = self.dropout(x)
        
        # Fully connected layer
        logits = self.fc(x)  # (batch_size, num_classes)
        
        return logits
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):
    model.to(device)
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for texts, labels in train_loader:
            texts, labels = texts.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')
        
        # Validation
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for texts, labels in val_loader:
                texts, labels = texts.to(device), labels.to(device)
                outputs = model(texts)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        
        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/total}')
# Hyperparameters
vocab_size = 10000  # Size of the vocabulary
embed_dim = 100  # Dimension of word embeddings
num_filters = 100  # Number of filters per filter size
filter_sizes = [3, 4, 5]  # Filter sizes
num_classes = 2  # Number of classes (e.g., binary classification)
dropout = 0.5  # Dropout rate
learning_rate = 0.001
num_epochs = 10
batch_size = 64

# Create dataset and dataloader
train_dataset = TextDataset(train_texts, train_labels, word_to_idx, max_len=50)
val_dataset = TextDataset(val_texts, val_labels, word_to_idx, max_len=50)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Initialize model, loss function, and optimizer
model = TextCNN(vocab_size, embed_dim, num_filters, filter_sizes, num_classes, dropout)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
